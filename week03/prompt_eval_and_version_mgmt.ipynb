{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5b333b",
   "metadata": {},
   "source": [
    "\n",
    "# Week03 — Prompt Evaluation & Version Management (Langfuse + Notion + GitHub)\n",
    "\n",
    "이 노트북은 **회사 회의 STT(회의록 텍스트) 1개**를 입력으로 받아,  \n",
    "- **V0.0.1 (간단 요약 Prompt)**, **V0.0.2 (회사형 구조화 회의록 Prompt)** 두 번의 Prompt Engineering을 수행하고,  \n",
    "- **Langfuse Tracing**을 남기며,  \n",
    "- **Langfuse Datasets**에 평가용 샘플 2개를 업로드하고,  \n",
    "- 이후 **Dataset Run/Evaluation**을 할 수 있도록 기본 코드를 제공합니다.\n",
    "\n",
    "> ⚙️ 이 노트북은 `.env`에 저장된 다음 변수들을 사용합니다.\n",
    "> - `LANGFUSE_PUBLIC_KEY`, `LANGFUSE_SECRET_KEY`, `LANGFUSE_HOST` (예: `https://cloud.langfuse.com` 또는 리전 URL)\n",
    "> - `OPENAI_API_KEY` (또는 호환 LLM API 키)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c1ccfc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.9 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q langfuse python-dotenv openai requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be11d3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded .env\n",
      "Langfuse host: https://cloud.langfuse.com\n",
      "Langfuse public key set?  True\n",
      "Langfuse secret key set?  True\n",
      "OpenAI key set?  True\n"
     ]
    }
   ],
   "source": [
    "# --- 0) 설치 & 기본 설정 ---------------------------------------------------------\n",
    "# 인터넷 환경에 따라 설치가 제한될 수 있습니다. 로컬 환경에서 실행하세요.\n",
    "# %pip install -q langfuse python-dotenv openai requests\n",
    "\n",
    "import os, json, time, uuid, sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "# .env 로드\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded .env\")\n",
    "except Exception as e:\n",
    "    print(\"python-dotenv not available; make sure environment variables are set.\")\n",
    "\n",
    "LANGFUSE_PUBLIC_KEY = os.getenv(\"LANGFUSE_PUBLIC_KEY\")\n",
    "LANGFUSE_SECRET_KEY = os.getenv(\"LANGFUSE_SECRET_KEY\")\n",
    "LANGFUSE_HOST = os.getenv(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Langfuse host:\", LANGFUSE_HOST)\n",
    "print(\"Langfuse public key set? \", bool(LANGFUSE_PUBLIC_KEY))\n",
    "print(\"Langfuse secret key set? \", bool(LANGFUSE_SECRET_KEY))\n",
    "print(\"OpenAI key set? \", bool(OPENAI_API_KEY))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae8ac2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded transcript: LLMOPS 기반 서비스 제작 논의.txt\n",
      "Length: 32345\n",
      "\n",
      "--- Preview (first 800 chars) ---\n",
      "\n",
      "﻿LLMOPS 기반 서비스 제작 논의\n",
      "2025.03.24 월 오후 2:06 ・ 71분 15초\n",
      "차성재\n",
      "\n",
      "\n",
      "참석자 1 00:11\n",
      "그리고 스테이만 오케니안 그다음에 리즘 디테일 트랜지션 컨플루션 그래머 있고 문장 센텐스 컴플렉스티 플렉스티 있고 그리고 스타일 이렇게 맞죠?\n",
      "스타일 맞나 원트 프리 프로 x 플리 마 텐 각각에 대해서 BDP가 있잖아요.\n",
      "그리고 이게 이게 이제 총 4가지로 해가지고 그 베이스 인터미리엣 지금 어드벤스 엑스프리가 엑스프레티 이렇게 있는 거고 총 요 로보인 게 똑같은 게 지금 내 네 세트가 있어요.\n",
      "그래서 이 부장님이 해 주시는 이제 여기서 이게 두 번째로 데이터 생성인데 이 부분이 오토메이션이 옵티메이션까지 안 간다 하더라도 지금 생각해 주시는 게 여기서 이거 갖고서 그러면 이 로그립에 맞는 우리가 데이터를 만들어주자라고 했을 때 그러니까 지금 총 10개의 평가 기준이 평가 항목이 있고 3개의 스케일이 있으니까 이것만 놓고 보면 지금 그렇게 안 하고 있어.\n",
      "\n",
      "참석자 1 01:41\n",
      "그러니까 이제 이 부분이 아마 정의가 좀 안 된 느낌이어 가지고 지금은 이 부장님이 해 주시는 거는 레벨 별로 그러니까 베이식이라는 거에 대해서 한 그때 아마 30개 정도 수주해 주셨나요?\n",
      "제가 20개 20개 근데 이 20개에 대한 매트릭을 이 부장님은 리카운트랑 무슨 램프랑 그리고\n",
      "\n",
      "참석자 2 02:05\n",
      "문단 수 수 아무튼 센텐스 카운트 어려운 단어 개수 AR 레벨 결국 AR 레벨 그게 이제 결과 값에 대한 리베이베이션이고 생성할 때는 베이직이랑 인터네이데이트 그거에 대한 상무님이 적어주신 파라미터 그거 반영해\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1) 회의 STT 텍스트 로드 ------------------------------------------------------\n",
    "transcript_path = \"LLMOPS 기반 서비스 제작 논의.txt\"\n",
    "# 노트북 실행 환경에 해당 파일이 없을 수 있으니 방어적으로 처리\n",
    "transcript_text = \"\"\n",
    "try:\n",
    "    with open(transcript_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript_text = f.read()\n",
    "        print(\"Loaded transcript:\", transcript_path)\n",
    "        print(\"Length:\", len(transcript_text))\n",
    "        print(\"\\n--- Preview (first 800 chars) ---\\n\")\n",
    "        print(transcript_text[:800])\n",
    "except FileNotFoundError:\n",
    "    print(\"Transcript file not found at:\", transcript_path)\n",
    "    transcript_text = \"회의 안건: LLM 기반 평가 파이프라인 정리, 루브릭 및 데이터셋 설계 논의. 참석자: A,B,C... (샘플 텍스트)\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cbc7c",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Prompts — V0.0.1 (간단 요약) vs V0.0.2 (구조화 회의록)\n",
    "\n",
    "- **V0.0.1**: 목적 중심의 간단 회의 요약 (한 문단 + bullet 5개 이하)  \n",
    "- **V0.0.2**: 회사용 구조화 회의록 (Decisions / Action Items / Key Points / Risks / Open Questions / Next Steps)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "975987be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "V001_SIMPLE_SUMMARY = '''You are a note-taker for a company meeting.\n",
    "Summarize the meeting transcript concisely for busy stakeholders.\n",
    "Output:\n",
    "- 1-paragraph abstract (<= 120 words)\n",
    "- Top-5 bullets of key points\n",
    "Be neutral and avoid speculation. Do not invent facts. Use present tense.\n",
    "'''\n",
    "\n",
    "V002_STRUCTURED_MINUTES = '''You are an expert corporate minute-taker.\n",
    "Extract structured minutes from the transcript with **no fabrication**.\n",
    "Return Markdown with the following sections (use headings exactly):\n",
    "# Decisions\n",
    "- [Decision] <concise statement> (owner if applicable)\n",
    "\n",
    "# Action Items\n",
    "- [Task] <what> — Owner: <name or \"TBD\"> — Due: <YYYY-MM-DD or \"TBD\">\n",
    "\n",
    "# Key Discussion Points\n",
    "- <point 1>\n",
    "- <point 2>\n",
    "\n",
    "# Risks\n",
    "- <risk 1 (cause → impact → mitigation)>\n",
    "\n",
    "# Open Questions\n",
    "- <question 1>\n",
    "\n",
    "# Next Steps\n",
    "- <step 1 (who/when)>\n",
    "\n",
    "Rules:\n",
    "- Quote numbers/dates only if present in the transcript; otherwise write \"TBD\".\n",
    "- Do not include PII beyond names mentioned.\n",
    "- Keep each bullet <= 25 words.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73550eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 3) LLM 호출 추상화 -----------------------------------------------------------\n",
    "def call_openai_chat(system_prompt: str, user_text: str, model: str = \"gpt-4o-mini\", temperature: float = 0.2) -> str:\n",
    "    \"\"\"LLM 호출. OPENAI_API_KEY가 없으면 모의 출력(mock)으로 대체.\"\"\"\n",
    "    if os.getenv(\"OPENAI_API_KEY\"):\n",
    "        try:\n",
    "            # Try official openai package first\n",
    "            try:\n",
    "                from openai import OpenAI\n",
    "                client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "                resp = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_text},\n",
    "                    ],\n",
    "                    temperature=temperature,\n",
    "                )\n",
    "                return resp.choices[0].message.content\n",
    "            except Exception:\n",
    "                # Fallback to raw HTTP if new package not available\n",
    "                import requests\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {os.getenv('OPENAI_API_KEY')}\",\n",
    "                    \"Content-Type\": \"application/json\",\n",
    "                }\n",
    "                payload = {\n",
    "                    \"model\": model,\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": system_prompt},\n",
    "                        {\"role\": \"user\", \"content\": user_text},\n",
    "                    ],\n",
    "                    \"temperature\": temperature,\n",
    "                }\n",
    "                r = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload, timeout=60)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                return data[\"choices\"][0][\"message\"][\"content\"]\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI call failed, falling back to mock. Error:\", e)\n",
    "    # Mock output for offline demo\n",
    "    return f\"\"\"[MOCK OUTPUT]\\nSystem: {system_prompt.splitlines()[0]}\\nUserInputPreview: {user_text[:120]}...\\n- Bullet 1\\n- Bullet 2\\n- Bullet 3\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec0eb1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langfuse SDK authenticated.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 4) Langfuse 초기화 & Tracing 유틸 -------------------------------------------\n",
    "USE_SDK = False\n",
    "langfuse = None\n",
    "\n",
    "try:\n",
    "    from langfuse import get_client, observe\n",
    "    from langfuse.openai import OpenAI as LFOpenAI  # optional\n",
    "    langfuse = get_client()\n",
    "    if langfuse and langfuse.auth_check():\n",
    "        USE_SDK = True\n",
    "        print(\"Langfuse SDK authenticated.\")\n",
    "    else:\n",
    "        print(\"Langfuse SDK not authenticated; will use HTTP or no-op.\")\n",
    "except Exception as e:\n",
    "    print(\"Langfuse SDK not available; continuing without it.\", e)\n",
    "\n",
    "def start_trace(name: str, metadata: Dict[str, Any] = None):\n",
    "    if USE_SDK:\n",
    "        # Start span as root trace context manager\n",
    "        return langfuse.start_as_current_span(name=name, metadata=metadata or {})\n",
    "    else:\n",
    "        # No-op context manager\n",
    "        from contextlib import contextmanager\n",
    "        @contextmanager\n",
    "        def _noop():\n",
    "            class Dummy:\n",
    "                def score_trace(self, *args, **kwargs): pass\n",
    "                def update_trace(self, *args, **kwargs): pass\n",
    "            yield Dummy()\n",
    "        return _noop()\n",
    "\n",
    "def log_trace_io(input_obj: Any, output_obj: Any):\n",
    "    if USE_SDK:\n",
    "        try:\n",
    "            # update current trace input/output to enable dataset evals linkage\n",
    "            langfuse.update_current_trace(input=input_obj, output=output_obj)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to update current trace:\", e)\n",
    "\n",
    "def flush_langfuse():\n",
    "    try:\n",
    "        if USE_SDK:\n",
    "            langfuse.flush()\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc6902f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== V0.0.1 (simple) ===\n",
      " **Abstract:**  \n",
      "The meeting discusses the development of an LLMOPS-based service, focusing on data generation and evaluation metrics. Participants outline the importance of defining rubrics and parameters for generating diverse outputs based on different complexity levels. They emphasize the need for automation in data creation and evaluation processes, ensuring that generated content meets specific criteria. The conversation highlights the challenges in defining metrics and the necessity for a structured approach to achieve varied outputs that align with educational standards.\n",
      "\n",
      "**Key Points:**\n",
      "- Discussion on LLMOPS service development, focusing on data generation and evaluation.\n",
      "- Importance of defining rubrics and parameters for various complexity levels.\n",
      "- Need for automation in data c ...\n",
      "\n",
      "=== V0.0.2 (structured) ===\n",
      " # Decisions\n",
      "- [Decision] Define evaluation metrics for LLMOPS service development (owner: TBD)\n",
      "\n",
      "# Action Items\n",
      "- [Task] Create a detailed rubric for evaluation metrics — Owner: TBD — Due: TBD\n",
      "- [Task] Develop automation pipeline for data generation — Owner: TBD — Due: TBD\n",
      "\n",
      "# Key Discussion Points\n",
      "- Importance of defining evaluation metrics for data generation.\n",
      "- Need for diverse variations in generated data for effective evaluation.\n",
      "\n",
      "# Risks\n",
      "- Lack of defined metrics may lead to ineffective evaluation (cause → unclear outcomes → establish clear metrics).\n",
      "\n",
      "# Open Questions\n",
      "- What specific parameters should be included in the evaluation rubric?\n",
      "\n",
      "# Next Steps\n",
      "- Discuss potential automation methods for data generation (who: TBD, when: TBD). ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 5) 두 가지 Prompt 실행 + Tracing --------------------------------------------\n",
    "subset = transcript_text[:4000]  # 데모를 위해 앞부분만 사용\n",
    "\n",
    "results = {}\n",
    "\n",
    "with start_trace(name=\"meeting-minutes:v0.0.1\", metadata={\"version\": \"v0.0.1\", \"label\": \"dev\", \"use_case\":\"meeting-minutes\"}) as span:\n",
    "    out1 = call_openai_chat(V001_SIMPLE_SUMMARY, subset)\n",
    "    results[\"v0.0.1\"] = out1\n",
    "    log_trace_io({\"transcript\": subset, \"prompt_version\": \"v0.0.1\"}, {\"minutes\": out1})\n",
    "\n",
    "with start_trace(name=\"meeting-minutes:v0.0.2\", metadata={\"version\": \"v0.0.2\", \"label\": \"staging\", \"use_case\":\"meeting-minutes\"}) as span:\n",
    "    out2 = call_openai_chat(V002_STRUCTURED_MINUTES, subset)\n",
    "    results[\"v0.0.2\"] = out2\n",
    "    log_trace_io({\"transcript\": subset, \"prompt_version\": \"v0.0.2\"}, {\"minutes\": out2})\n",
    "\n",
    "flush_langfuse()\n",
    "\n",
    "print(\"\\n=== V0.0.1 (simple) ===\\n\", results[\"v0.0.1\"][:800], \"...\")\n",
    "print(\"\\n=== V0.0.2 (structured) ===\\n\", results[\"v0.0.2\"][:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffda36",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Langfuse Dataset 생성 & 아이템 업로드 (샘플 2개)\n",
    "\n",
    "- 회의록 일부를 input으로, 기대 구조를 reference로 저장합니다.  \n",
    "- 이후 Langfuse에서 **Dataset Run**을 실행하여, 각 Prompt 버전의 결과를 비교/평가할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "144d0eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset 'week03_meeting_minutes_demo' with 2 items via SDK.\n",
      "Local backup dataset written to /Users/hajuheon/Task/ai/ajou/25-2/llmops/week03/datasets/week03_meeting_minutes_demo.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dataset 이름\n",
    "DATASET_NAME = \"news_summary\"\n",
    "\n",
    "# 샘플 아이템 2개 (간단 예시)\n",
    "item1 = {\n",
    "    \"input\": {\"transcript\": subset[:1500]},\n",
    "    \"expected_output\": {\n",
    "        \"sections\": [\"Decisions\",\"Action Items\",\"Key Discussion Points\",\"Next Steps\"],  # 기대 섹션 존재성\n",
    "    }\n",
    "}\n",
    "item2 = {\n",
    "    \"input\": {\"transcript\": subset[1500:3000] if len(subset) > 3000 else subset},\n",
    "    \"expected_output\": {\n",
    "        \"sections\": [\"Decisions\",\"Action Items\",\"Risks\",\"Open Questions\",\"Next Steps\"],\n",
    "    }\n",
    "}\n",
    "\n",
    "created = False\n",
    "if USE_SDK:\n",
    "    try:\n",
    "        langfuse.create_dataset(name=DATASET_NAME)\n",
    "        for item in (item1, item2):\n",
    "            langfuse.create_dataset_item(\n",
    "                dataset_name=DATASET_NAME,\n",
    "                input=item[\"input\"],\n",
    "                expected_output=item[\"expected_output\"]\n",
    "            )\n",
    "        created = True\n",
    "        print(f\"Created dataset '{DATASET_NAME}' with 2 items via SDK.\")\n",
    "    except Exception as e:\n",
    "        print(\"SDK dataset creation failed:\", e)\n",
    "\n",
    "# 로컬 JSONL도 함께 저장(백업/수동 업로드용)\n",
    "ds_dir = Path(\"datasets\"); ds_dir.mkdir(exist_ok=True)\n",
    "jsonl_path = ds_dir / f\"{DATASET_NAME}.jsonl\"\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in (item1, item2):\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "print(\"Local backup dataset written to\", jsonl_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f56eb65",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (선택) Native Dataset Run 실행 예시\n",
    "\n",
    "아래 코드는 Langfuse **Datasets Cookbook**의 패턴을 따라, 각 Dataset Item을 순회하며 애플리케이션을 실행하고  \n",
    "`root_span.score_trace(...)` 등으로 평가 스코어를 기록합니다.  \n",
    "실행 전, SDK 인증이 되어 있어야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f02db63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished dataset run 'minutes_v0.0.2' on dataset 'week03_meeting_minutes_demo'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def presence_of_sections_eval(output_text: str, expected_sections) -> float:\n",
    "    \"\"\"출력에 예상 섹션 헤더가 얼마나 포함되는지 0.0~1.0 반환.\"\"\"\n",
    "    if not output_text:\n",
    "        return 0.0\n",
    "    hits = 0\n",
    "    for sec in expected_sections:\n",
    "        if f\"# {sec}\" in output_text or sec in output_text:\n",
    "            hits += 1\n",
    "    return hits / max(1, len(expected_sections))\n",
    "\n",
    "def run_dataset_experiment(run_name: str, system_prompt: str):\n",
    "    if not USE_SDK:\n",
    "        print(\"Langfuse SDK unavailable; skipping remote run.\")\n",
    "        return\n",
    "    dataset = langfuse.get_dataset(DATASET_NAME)\n",
    "    for item in dataset.items:\n",
    "        with item.run(run_name=run_name) as root_span:\n",
    "            output = call_openai_chat(system_prompt, item.input[\"transcript\"])\n",
    "            # Trace IO를 업데이트해야 Langfuse의 Eval 기능에서 인풋/아웃풋을 인식합니다.\n",
    "            root_span.update_trace(input=item.input, output=output)\n",
    "            # 간단한 구조성 스코어 예시\n",
    "            score = presence_of_sections_eval(output, item.expected_output.get(\"sections\", []))\n",
    "            root_span.score_trace(name=\"section_coverage\", value=score)\n",
    "    langfuse.flush()\n",
    "    print(f\"Finished dataset run '{run_name}' on dataset '{DATASET_NAME}'.\")\n",
    "\n",
    "# 예시 실행 (주석 해제하여 사용)\n",
    "run_dataset_experiment(run_name=\"minutes_v0.0.2\", system_prompt=V002_STRUCTURED_MINUTES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bdd895",
   "metadata": {},
   "source": [
    "\n",
    "## 8) (선택) Langfuse Prompt 버전 생성 & 라벨 지정 (REST API 예시)\n",
    "\n",
    "- 동일 `promptName`으로 새 버전을 생성하면 자동으로 버전이 증분됩니다.  \n",
    "- `labels`에 `staging`, `production` 등을 부여/변경하여 배포 포인터로 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b72665ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1027f4b2-8e08-442c-adfe-9ea5f2d2f8fa',\n",
       " 'createdAt': '2025-09-20T18:44:56.767Z',\n",
       " 'updatedAt': '2025-09-20T18:44:56.767Z',\n",
       " 'projectId': 'cmfl1scks01diad07dc3azk31',\n",
       " 'createdBy': 'API',\n",
       " 'prompt': 'You are an expert corporate minute-taker.\\nExtract structured minutes from the transcript with **no fabrication**.\\nReturn Markdown with the following sections (use headings exactly):\\n# Decisions\\n- [Decision] <concise statement> (owner if applicable)\\n\\n# Action Items\\n- [Task] <what> — Owner: <name or \"TBD\"> — Due: <YYYY-MM-DD or \"TBD\">\\n\\n# Key Discussion Points\\n- <point 1>\\n- <point 2>\\n\\n# Risks\\n- <risk 1 (cause → impact → mitigation)>\\n\\n# Open Questions\\n- <question 1>\\n\\n# Next Steps\\n- <step 1 (who/when)>\\n\\nRules:\\n- Quote numbers/dates only if present in the transcript; otherwise write \"TBD\".\\n- Do not include PII beyond names mentioned.\\n- Keep each bullet <= 25 words.\\n',\n",
       " 'name': 'meeting-minutes',\n",
       " 'version': 2,\n",
       " 'type': 'text',\n",
       " 'isActive': None,\n",
       " 'config': {'model_name': 'gpt-4o-mini', 'temperature': 0.2},\n",
       " 'tags': [],\n",
       " 'labels': ['production', 'latest'],\n",
       " 'commitMessage': None}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import base64, json\n",
    "\n",
    "def lf_auth_headers():\n",
    "    token = base64.b64encode(f\"{LANGFUSE_PUBLIC_KEY}:{LANGFUSE_SECRET_KEY}\".encode()).decode()\n",
    "    return {\"Authorization\": f\"Basic {token}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "def create_prompt_version_via_rest(prompt_name: str, prompt_text: str, model_name: str, labels=None):\n",
    "    if not (LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY):\n",
    "        print(\"Missing Langfuse credentials.\")\n",
    "        return None\n",
    "    import requests\n",
    "    url = f\"{LANGFUSE_HOST.rstrip('/')}/api/public/v2/prompts\"\n",
    "    payload = {\n",
    "        \"name\": prompt_name,\n",
    "        \"prompt\": prompt_text,\n",
    "        \"config\": {\"model_name\": model_name, \"temperature\": 0.2},\n",
    "        \"labels\": labels or []\n",
    "    }\n",
    "    r = requests.post(url, headers=lf_auth_headers(), json=payload, timeout=30)\n",
    "    try:\n",
    "        r.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(\"Prompt creation failed:\", r.status_code, r.text)\n",
    "        raise\n",
    "    return r.json()\n",
    "\n",
    "# 예시: v0.1 → v0.2 (staging → production) 업로드 (주석 해제 후 사용)\n",
    "create_prompt_version_via_rest(\"meeting-minutes\", V001_SIMPLE_SUMMARY, \"gpt-4o-mini\", labels=[\"staging\"])\n",
    "create_prompt_version_via_rest(\"meeting-minutes\", V002_STRUCTURED_MINUTES, \"gpt-4o-mini\", labels=[\"production\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9552155",
   "metadata": {},
   "source": [
    "\n",
    "## 9) Prompty 파일 생성 (GitHub PR용)\n",
    "\n",
    "- `prompts/meeting_minutes_v0.1.prompty`  \n",
    "- `prompts/meeting_minutes_v0.2.prompty`  \n",
    "를 생성합니다. (PR 시 `v0.1 → v0.2` 변경 경험 포함)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669aaa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "print(\"Prompty files:\")\n",
    "for p in Path(\"week03/prompts\").glob(\"*.prompty\"):\n",
    "    print(\"-\", p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c93ce",
   "metadata": {},
   "source": [
    "\n",
    "## 10) 마무리 & 다음 단계\n",
    "1. **Tracing 확인**: Langfuse 콘솔에서 오늘 생성된 트레이스를 확인합니다.  \n",
    "2. **Dataset Run**: `run_dataset_experiment(...)` 실행 후 지표(예: `section_coverage`) 확인.  \n",
    "3. **Prompt 배포 라벨**: REST 또는 UI로 `staging` → `production` 라벨 전환.  \n",
    "4. **GitHub PR**: `prompts/meeting_minutes_v0.1.prompty` → `v0.2` 변경 포함 PR 생성.  \n",
    "5. **초대**: 프로젝트에 `smilechacha@ajou.ac.kr` 뷰어 이상 권한 초대.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
